{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "privacy_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuuhK2cZsLTL"
      },
      "source": [
        "!git clone https://github.com/lliu12/cpdefense.git \n",
        "%cd cpdefense\n",
        "\n",
        "from leakage_utils import *\n",
        "from training_utils import *\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import sys\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFt0qylstVD8"
      },
      "source": [
        "# Using CIFAR-10\n",
        "# training data\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
        "                                        download=True,\n",
        "                                        transform=transforms.ToTensor())\n",
        "\n",
        "# testing data\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True,\n",
        "                                       transform=transforms.ToTensor())\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB6ySBw2xwTQ"
      },
      "source": [
        "c = 9 # num clusters\n",
        "n = 9 # num PCA dimensions to project onto\n",
        "num_devices = 60 # use 60 for actual experiments\n",
        "\n",
        "net = DLGNet().cuda()\n",
        "training_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "leakage_criterion = cross_entropy_for_onehot\n",
        "# leakage_optimizer = torch.optim.LBFGS([random_image, random_label], lr = 1)\n",
        "\n",
        "init_rounds = 1\n",
        "local_epochs = 3\n",
        "num_items_per_device = 5000\n",
        "device_nums = [1, 1, 1]\n",
        "data_idxs = iid_sampler(trainset, num_devices, 0.1)\n",
        "devices = []\n",
        "for i in data_idxs: # make devices\n",
        "    new_d = create_device(net, i, trainset, data_idxs[i],\n",
        "                        milestones=[250, 500, 750], batch_size=128)\n",
        "    devices.append(new_d)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "188QiuIS0aw-"
      },
      "source": [
        "for round_num in range(init_rounds):\n",
        "    round_devices = devices\n",
        "    print('Round: ', round_num)\n",
        "    for device in round_devices:\n",
        "        for local_epoch in range(local_epochs):\n",
        "            train(local_epoch, device, net)\n",
        "\n",
        "    w_avg = average_weights(round_devices)\n",
        "\n",
        "    for device in devices:\n",
        "        device['net'].load_state_dict(w_avg)\n",
        "        device['optimizer'].zero_grad()\n",
        "        device['optimizer'].step()\n",
        "        device['scheduler'].step()\n",
        "\n",
        "    test(round_num, devices[0], net, testloader)\n",
        "    \n",
        "    arr = []\n",
        "    for i in range(len(devices)):\n",
        "        new_torch = torch.empty([1]).cuda()\n",
        "        for k in devices[i]['weights']:\n",
        "            if 'weight'  in k:\n",
        "                new_torch = torch.cat((new_torch, devices[i]['weights'][k].flatten()), 0)\n",
        "        arr.append(new_torch.detach().cpu().numpy())\n",
        "\n",
        "    pca = PCA(n_components=n)\n",
        "    X_train_pca = pca.fit_transform(arr)\n",
        "knn = KMeans(n_clusters=c).fit(X_train_pca) \n",
        "preds = knn.predict(X_train_pca)\n",
        "clusters = get_cluster_dict(preds)\n",
        "print(preds)\n",
        "print(clusters)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLfuNQDISWKg"
      },
      "source": [
        "# # alternative code for evenly sized clusters\n",
        "# preds = []\n",
        "# for i in range(c): # assumes c = 9\n",
        "#   num_in_cluster = 7 if i < 6 else 6\n",
        "#   preds = preds + [i] * num_in_cluster\n",
        "# len(preds)\n",
        "# clusters = get_cluster_dict(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIo2AY9OfYi7"
      },
      "source": [
        "# now that clusters have been obtained, reset the NN weights to unif dist\n",
        "# set all devices to same model a la FL\n",
        "\n",
        "overlap_factor = 1\n",
        "\n",
        "devices[0]['net'].apply(weights_init)\n",
        "for d in devices:\n",
        "  d['net'].load_state_dict(devices[0]['net'].state_dict())\n",
        "\n",
        "device_gradients = [{} for _ in range(len(devices))]\n",
        "for i, d in enumerate(devices):\n",
        "  sample_image, onehot_sample_label = get_device_data_sample(d)\n",
        "  grad = get_true_device_gradient(d, sample_image, onehot_sample_label)\n",
        "  device_gradients[i]['sample_image'] = sample_image\n",
        "  device_gradients[i]['onehot_sample_label'] = onehot_sample_label\n",
        "  device_gradients[i]['orig_grad'] = grad\n",
        "\n",
        "add_pruned_gradients(device_gradients, clusters, overlap_factor, amplify_factor = 1)\n",
        "\n",
        "cluster_gradients = get_cluster_gradients(device_gradients, clusters, for_attack=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bvw7BF9r6Kx"
      },
      "source": [
        "# test individual privacy: actual experiment\n",
        "# leakage code adapted from https://github.com/mit-han-lab/dlg/blob/master/main.py\n",
        "\n",
        "device_individual_privacy_results = [{} for _ in devices]\n",
        "for d_num in range(len(devices)):\n",
        "  if d_num % 5 == 0:\n",
        "    print(\"Working on device \" + str(d_num) + \"...\")\n",
        "\n",
        "  sample_image = device_gradients[d_num]['sample_image'] # target image to leak\n",
        "  sample_label = device_gradients[d_num]['onehot_sample_label']\n",
        "  device_individual_privacy_results[d_num]['best_psnr'] = -np.inf\n",
        "  device_individual_privacy_results[d_num]['label_leaked_count'] = 0\n",
        "  device_individual_privacy_results[d_num]['attempts_count'] = 0\n",
        "\n",
        "  for _ in range(5): # 5 tries to leak the image\n",
        "    device_individual_privacy_results[d_num]['attempts_count'] += 1\n",
        "    random_image, random_label, history = try_recovery_individual(devices[d_num], device_gradients[d_num]['pruned_grad'])\n",
        "    psnrs = [psnr(sample_image, im) for im in history]\n",
        "    psnr_max_this_trial = np.max(psnrs)\n",
        "    # update best psnr seen so far for this device\n",
        "    device_individual_privacy_results[d_num]['best_psnr'] = np.max([device_individual_privacy_results[d_num]['best_psnr'], psnr_max_this_trial])\n",
        "    # check if sample label was leaked by final iteration\n",
        "    if torch.argmax(device_gradients[d_num]['onehot_sample_label']) == torch.argmax(random_label):\n",
        "      device_individual_privacy_results[d_num]['label_leaked_count'] += 1\n",
        "\n",
        "# print results\n",
        "\n",
        "best_psnrs = [device_individual_privacy_results[i]['best_psnr'] for i in range(len(devices))]\n",
        "print(\"Max PSNR: \" + str(np.max(best_psnrs)))\n",
        "print(\"Mean PSNR: \" + str(np.mean(best_psnrs)))\n",
        "\n",
        "label_leaks = [device_individual_privacy_results[i]['label_leaked_count'] for i in range(len(devices))]\n",
        "total_label_leaks = np.sum(label_leaks)\n",
        "total_attempts = 5 * len(devices)\n",
        "print(\"Percentage of Labels Leaked: \" + str(total_label_leaks / total_attempts))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hP-aYz_6Wq9"
      },
      "source": [
        "# launch an attack on a specific device\n",
        "# leakage works best for a device that's the only device in its cluster\n",
        "\n",
        "d_num = 24\n",
        "im, lab, hist = try_recovery_individual(devices[d_num], device_gradients[d_num]['pruned_grad'])\n",
        "print(\"Recovered PSNR: \")\n",
        "psnr(im, device_gradients[d_num]['sample_image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ivrPo8lBp2y"
      },
      "source": [
        "# print the progression of optimization for this device\n",
        "plot_history(hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdjfUrZbBrXK"
      },
      "source": [
        "# plot final recovered image\n",
        "\n",
        "plt.imshow(np.transpose(im.detach().numpy(), (1,2,0)))\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2esv6PPZBz0X"
      },
      "source": [
        "# plot true original device image\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "plt.imshow(np.transpose(device_gradients[d_num]['sample_image'], (1,2,0)))\n",
        "plt.axis('off')\n",
        "sample_label = device_gradients[d_num]['onehot_sample_label']\n",
        "print(\"Label: \" + str(sample_label) + \", a \" + classes[torch.argmax(sample_label)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsdqX4biYMTL"
      },
      "source": [
        " # get the image and label pair stored for a device\n",
        "def get_device_pair(d_num):\n",
        "  return device_gradients[d_num]['sample_image'], device_gradients[d_num]['onehot_sample_label']\n",
        " \n",
        " # try manually setting devices in desired clusters to certain images \n",
        " # just an example!\n",
        " # (will need to be redone for new clusters/data distributions)\n",
        " cluster_attack_device_gradients = [{} for _ in range(len(devices))]\n",
        "for i, d in enumerate(devices):\n",
        "  if i == 53 or i == 59 or i == 40:\n",
        "    sample_image, onehot_sample_label = get_device_pair(2)\n",
        "  elif i == 8 or i == 44 or i == 26:\n",
        "    sample_image, onehot_sample_label = get_device_pair(16)\n",
        "  elif i == 0 or i == 52:\n",
        "    sample_image, onehot_sample_label = get_device_pair(5)\n",
        "  elif i == 30:\n",
        "    sample_image, onehot_sample_label = get_device_pair(15)\n",
        "  else:\n",
        "    sample_image, onehot_sample_label = get_device_pair(26)\n",
        "  grad = get_true_device_gradient(d, sample_image, onehot_sample_label)\n",
        "  cluster_attack_device_gradients[i]['sample_image'] = sample_image\n",
        "  cluster_attack_device_gradients[i]['onehot_sample_label'] = onehot_sample_label\n",
        "  cluster_attack_device_gradients[i]['orig_grad'] = grad\n",
        "\n",
        "add_pruned_gradients(cluster_attack_device_gradients, clusters, overlap_factor, amplify_factor = 1)\n",
        "\n",
        "cluster_attack_cluster_gradients = get_cluster_gradients(cluster_attack_device_gradients, clusters, for_attack=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w20VahgAINgj"
      },
      "source": [
        "# test cluster privacy\n",
        "# will take a lot of runs to get leakage :)\n",
        "random_image, random_label = get_random_pair()\n",
        "\n",
        "# make optimizer\n",
        "leakage_optimizer = torch.optim.LBFGS([random_image, random_label], lr = 1)\n",
        "\n",
        "select_cluster = 0 # set cluster to attack\n",
        "actual_grad = copy.deepcopy(cluster_attack_cluster_gradients[select_cluster])\n",
        "\n",
        "history = []\n",
        "num_iters = 100\n",
        "save_every = 10\n",
        "\n",
        "for iters in range(num_iters):\n",
        "  def cluster_closure():\n",
        "    leakage_optimizer.zero_grad()\n",
        "    # need to do these lines below for all devices in the cluster using random image and populate somewhere in device_gradients\n",
        "    dummy_preds = {}\n",
        "    dummy_onehot_labels = {}\n",
        "    dummy_losses = {}\n",
        "    for i, select_device in enumerate(clusters[select_cluster]):\n",
        "      dummy_preds[i] = devices[select_device]['net'](torch.unsqueeze(random_image, dim = 0).cuda())\n",
        "      dummy_onehot_labels[i] = F.softmax(random_label, dim=-1).cuda()\n",
        "      dummy_losses[i] = leakage_criterion(dummy_preds[i], dummy_onehot_labels[i]) \n",
        "\n",
        "      cluster_attack_device_gradients[select_device][\"grad_for_attack\"] = torch.autograd.grad(dummy_losses[i], devices[select_device]['net'].parameters(), create_graph = True)\n",
        "\n",
        "    # then...\n",
        "    prune_attack_gradients(cluster_attack_device_gradients, clusters[select_cluster], overlap_factor, amplify_factor = 1)\n",
        "    dummy_grad = get_cluster_gradient(select_cluster, cluster_attack_device_gradients, clusters, for_attack = True)\n",
        "\n",
        "    grad_diff = 0\n",
        "    for gx, gy in zip(dummy_grad, actual_grad): \n",
        "      grad_diff += ((gx - gy) ** 2).sum()\n",
        "    grad_diff.backward()\n",
        "    return grad_diff\n",
        "  leakage_optimizer.step(cluster_closure)\n",
        "  if iters % save_every == 0:\n",
        "    diff = cluster_closure()\n",
        "    print(diff)\n",
        "    history.append(copy.deepcopy(random_image.cpu().detach()))\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9im1x9eMTfXw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}